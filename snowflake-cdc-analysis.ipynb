{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f846ea-b4b0-4657-b499-cd8b5b1eb380",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_select_pkg",
    "resultHeight": 275
   },
   "source": [
    "# Select Packages\n",
    "\n",
    "To get started, let's select a few packages that we will need. In the **Packages** drop-down picker in the top right of the UI, search for and add the following packages by clicking on them:\n",
    "\n",
    "- snowflake-ml-python\n",
    "\n",
    "Once you add the packages, click the **Start** button! Once it says **Active**, you're ready to run the rest of the Notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f4adaa3-8ef6-4c8f-8057-1d12bf900962",
   "metadata": {
    "name": "md_data_ingest",
    "collapsed": false,
    "resultHeight": 230
   },
   "source": "## 1. Data Ingestion\n\nThe data for this notebook should have been ingested into a table called cdc_data, following the steps outlined in the Example ML Flow Workload found in our roachlong public repositories on GitHub.  This data represents simulated credit card transactions and is a fairly simple dataset for the purposes of creating an end-to-end process of capturing, publishing and anlayzing high volume transactional data in near real-time.  We'll use this notebook to explore some of the data.\n\n"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fe2a4b4-821f-410c-8d2c-e527829b106e",
   "metadata": {
    "name": "md_import_libs",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed4494-06ab-43b2-86e8-879c99c1a2c0",
   "metadata": {
    "language": "python",
    "name": "import_libs",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Snowpark for Python\nfrom snowflake.snowpark.types import DoubleType\nimport snowflake.snowpark.functions as F\nimport numpy as np\nimport pandas as pd"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39326625-0d17-438f-a847-8d1e2c1488da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_active_session",
    "resultHeight": 139
   },
   "source": "### Setup and establish Secure Connection to Snowflake\n\nNotebooks establish a Snowpark Session when the notebook is attached to the kernel. We're using a trial acocunt for this tutorial and leveraging the x-small COMPUTE_WH that comes with it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76605036-bbf0-45cd-ac16-8c7bb391384c",
   "metadata": {
    "language": "sql",
    "name": "init_sql",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "-- Using Warehouse, Database, and Schema created during Setup\nUSE WAREHOUSE COMPUTE_WH;\nUSE DATABASE PAYMENTS;\nUSE SCHEMA PUBLIC;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fa3ea-5ea9-4af6-a4dd-88c7101dcf0d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "get_active_session",
    "resultHeight": 150
   },
   "outputs": [],
   "source": [
    "# Get Snowflake Session object\n",
    "session = get_active_session()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "# Add a query tag to the session.\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"e2e_ml_snowparkpython\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0,},\n",
    "                     \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\"}}\n",
    "\n",
    "# Current Environment Details\n",
    "print('Connection Established with the following parameters:')\n",
    "print('User      : {}'.format(session.get_current_user()))\n",
    "print('Role      : {}'.format(session.get_current_role()))\n",
    "print('Database  : {}'.format(session.get_current_database()))\n",
    "print('Schema    : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse : {}'.format(session.get_current_warehouse()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9f22ce6-da50-4d76-98e1-3ff6246ed220",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_read_cdc_data",
    "resultHeight": 369
   },
   "source": "### Use the Snowpark DataFrame Reader to read in data from the internally ingested cdc_data table\n\nIn our Sample ML Flow Workload, we staged CDC generated JSON files from an external s3 bucket. Then transformed and auto-loaded the data into a table using a Snowflake pipe.\n\nNext, for each source table,\n* we'll parse the CDC data based on file type,\n* match each record to a respective resolve timestamp,\n* and reduce the records per resolve time to the latest updates\n\nFor more information on loading data, see documentation on [snowflake.snowpark.DataFrameReader](https://docs.snowflake.com/ko/developer-guide/snowpark/reference/python/api/snowflake.snowpark.DataFrameReader.html).\n\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080dc92-595e-48b6-b4fc-667e2346bed9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "read_address_data",
    "codeCollapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# Create a Snowpark DataFrame using a sql query to parse JSON fields\n# And match records to a resolved time based on ingested file timestamp\naddr_df = session.sql(\"\"\"\n    with checkpoint as (\n        select data:resolved::varchar(32) as resolved,\n               split_part(filename, '.', 1) as timestamp\n        from cdc_data\n        where filename like '%.RESOLVED'\n    )\n    select cdc.data:after:id::varchar(64) as addr_id,\n           cdc.data:after:acct_num::varchar(19) as acct_num,\n           cdc.data:after:street::varchar(64) as street,\n           cdc.data:after:zip::varchar(9) as zip,\n           cdc.data:after:lat::number(11, 8) as lat,\n           cdc.data:after:lng::number(11, 8) as lng,\n           cp.resolved as cp_resolved,\n           cp.timestamp as cp_timestamp,\n           split_part(cdc.filename, '-', 1) as file_timestamp,\n           cdc.data:mvcc_timestamp::varchar(32) as mvcc_timestamp,\n           cdc.data:updated::varchar(32) as updated,\n           cdc.file_row_number as file_row_number\n    from cdc_data cdc, checkpoint cp\n    where cdc.filename like '%-address-%'\n      and cp.timestamp = (select min(timestamp)\n                          from checkpoint\n                          where timestamp >= split_part(cdc.filename, '-', 1))\n      and file_timestamp <= cp.timestamp\n      and file_timestamp > (select max(timestamp)\n                            from checkpoint\n                            where timestamp < split_part(cdc.filename, '-', 1))\n\"\"\")\naddr_df.limit(5)"
  },
  {
   "cell_type": "code",
   "id": "59c7fdb8-142c-4db7-9298-807a0c9f1d2f",
   "metadata": {
    "language": "python",
    "name": "reduce_address_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# We'll reduce the dataframe to only inlcude the latest update\n# associated with a single record per each resolve timestamp\n\n# We need a pandas dataframe in order use the .loc and groupby functions\naddr_pd = addr_df.to_pandas()\n\n# And get the latest record per ID and resolve time in a single file\naddr_pd = addr_pd.loc[addr_pd\n    .groupby(\n        ['ADDR_ID', 'CP_RESOLVED', 'FILE_TIMESTAMP']\n    )['FILE_ROW_NUMBER']\n    .agg(pd.Series.idxmax)]\n\n# Then reduce to the latest record across multiple files\naddr_pd = addr_pd.loc[addr_pd\n    .groupby(\n        ['ADDR_ID', 'CP_RESOLVED']\n    )['FILE_TIMESTAMP']\n    .agg(pd.Series.idxmax)]\n\n# Now our data should be unique by the entity ID and CP_RESOLVED\n# And we'll set the index to later join with transaction data\naddr_pd.set_index(['ACCT_NUM', 'CP_RESOLVED'], inplace=True)\n\naddr_pd.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cc8c048-0942-4484-ae86-3095b7492153",
   "metadata": {
    "language": "python",
    "name": "read_city_data",
    "codeCollapsed": false,
    "collapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# Create a Snowpark DataFrame using a sql query to parse JSON fields\n# And match records to a resolved time based on ingested file timestamp\ncity_df = session.sql(\"\"\"\n    with checkpoint as (\n        select data:resolved::varchar(32) as resolved,\n               split_part(filename, '.', 1) as timestamp\n        from cdc_data\n        where filename like '%.RESOLVED'\n    )\n    select cdc.data:after:id::varchar(64) as city_id,\n           cdc.data:after:zip::varchar(9) as zip,\n           cdc.data:after:city::varchar(32) as city,\n           cdc.data:after:state::varchar(2) as state,\n           cdc.data:after:city_pop::number(9, 0) as city_pop,\n           cp.resolved as cp_resolved,\n           cp.timestamp as cp_timestamp,\n           split_part(cdc.filename, '-', 1) as file_timestamp,\n           cdc.data:mvcc_timestamp::varchar(32) as mvcc_timestamp,\n           cdc.data:updated::varchar(32) as updated,\n           cdc.file_row_number as file_row_number\n    from cdc_data cdc, checkpoint cp\n    where cdc.filename like '%-city_loc-%'\n      and cp.timestamp = (select min(timestamp)\n                          from checkpoint\n                          where timestamp >= split_part(cdc.filename, '-', 1))\n      and file_timestamp <= cp.timestamp\n      and file_timestamp > (select max(timestamp)\n                            from checkpoint\n                            where timestamp < split_part(cdc.filename, '-', 1))\n\"\"\")\ncity_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4419bbd8-2e59-4596-84f8-bb2903bd0d35",
   "metadata": {
    "language": "python",
    "name": "reduce_city_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# We'll reduce the dataframe to only inlcude the latest update\n# associated with a single record per each resolve timestamp\n\n# We need a pandas dataframe in order use the .loc and groupby functions\ncity_pd = city_df.to_pandas()\n\n# And get the latest record per ID and resolve time in a single file\ncity_pd = city_pd.loc[city_pd\n    .groupby(\n        ['CITY_ID', 'CP_RESOLVED', 'FILE_TIMESTAMP']\n    )['FILE_ROW_NUMBER']\n    .agg(pd.Series.idxmax)]\n\n# Then reduce to the latest record across multiple files\ncity_pd = city_pd.loc[city_pd\n    .groupby(\n        ['CITY_ID', 'CP_RESOLVED']\n    )['FILE_TIMESTAMP']\n    .agg(pd.Series.idxmax)]\n\n# Now our data should be unique by the entity ID and CP_RESOLVED\n# And we'll set the index to later join with transaction data\ncity_pd.set_index(['ZIP', 'CP_RESOLVED'], inplace=True)\n\ncity_pd.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2dba4138-6978-40e9-9ba5-66626b99bd52",
   "metadata": {
    "language": "python",
    "name": "read_customer_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# Create a Snowpark DataFrame using a sql query to parse JSON fields\n# And match records to a resolved time based on ingested file timestamp\ncust_df = session.sql(\"\"\"\n    with checkpoint as (\n        select data:resolved::varchar(32) as resolved,\n               split_part(filename, '.', 1) as timestamp\n        from cdc_data\n        where filename like '%.RESOLVED'\n    )\n    select cdc.data:after:id::varchar(64) as cust_id,\n           cdc.data:after:ssn::varchar(16) as ssn,\n           cdc.data:after:cc_num::varchar(19) as cc_num,\n           cdc.data:after:first::varchar(16) as first,\n           cdc.data:after:last::varchar(16) as last,\n           cdc.data:after:gender::varchar(1) as gender,\n           cdc.data:after:job::varchar(64) as job,\n           cdc.data:after:dob::date as dob,\n           cdc.data:after:acct_num::varchar(19) as acct_num,\n           cdc.data:after:profile::varchar(32) as profile,\n           cp.resolved as cp_resolved,\n           cp.timestamp as cp_timestamp,\n           split_part(cdc.filename, '-', 1) as file_timestamp,\n           cdc.data:mvcc_timestamp::varchar(32) as mvcc_timestamp,\n           cdc.data:updated::varchar(32) as updated,\n           cdc.file_row_number as file_row_number\n    from cdc_data cdc, checkpoint cp\n    where cdc.filename like '%-customer-%'\n      and cp.timestamp = (select min(timestamp)\n                          from checkpoint\n                          where timestamp >= split_part(cdc.filename, '-', 1))\n      and file_timestamp <= cp.timestamp\n      and file_timestamp > (select max(timestamp)\n                            from checkpoint\n                            where timestamp < split_part(cdc.filename, '-', 1))\n\"\"\")\ncust_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25764dda-c895-4942-80c2-f87b3e6cc430",
   "metadata": {
    "language": "python",
    "name": "reduce_customer_data",
    "codeCollapsed": false,
    "collapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# We'll reduce the dataframe to only inlcude the latest update\n# associated with a single record per each resolve timestamp\n\n# We need a pandas dataframe in order use the .loc and groupby functions\ncust_pd = cust_df.to_pandas()\n\n# And get the latest record per ID and resolve time in a single file\ncust_pd = cust_pd.loc[cust_pd\n    .groupby(\n        ['CUST_ID', 'CP_RESOLVED', 'FILE_TIMESTAMP']\n    )['FILE_ROW_NUMBER']\n    .agg(pd.Series.idxmax)]\n\n# Then reduce to the latest record across multiple files\ncust_pd = cust_pd.loc[cust_pd\n    .groupby(\n        ['CUST_ID', 'CP_RESOLVED']\n    )['FILE_TIMESTAMP']\n    .agg(pd.Series.idxmax)]\n\n# Now our data should be unique by the entity ID and CP_RESOLVED\n# And we'll set the index to later join with transaction data\ncust_pd.set_index(['CC_NUM', 'CP_RESOLVED'], inplace=True)\n\ncust_pd.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d4ce56e-1547-45b7-b69b-cac5de4b1a4e",
   "metadata": {
    "language": "python",
    "name": "read_merchant_data",
    "codeCollapsed": false,
    "collapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# Create a Snowpark DataFrame using a sql query to parse JSON fields\n# And match records to a resolved time based on ingested file timestamp\nmerc_df = session.sql(\"\"\"\n    with checkpoint as (\n        select data:resolved::varchar(32) as resolved,\n               split_part(filename, '.', 1) as timestamp\n        from cdc_data\n        where filename like '%.RESOLVED'\n    )\n    select cdc.data:after:id::varchar(64) as merch_id,\n           cdc.data:after:merchant::varchar(64) as merchant,\n           cdc.data:after:merch_lat::number(11, 8) as merch_lat,\n           cdc.data:after:merch_lng::number(11, 8) as merch_lng,\n           cp.resolved as cp_resolved,\n           cp.timestamp as cp_timestamp,\n           split_part(cdc.filename, '-', 1) as file_timestamp,\n           cdc.data:mvcc_timestamp::varchar(32) as mvcc_timestamp,\n           cdc.data:updated::varchar(32) as updated,\n           cdc.file_row_number as file_row_number\n    from cdc_data cdc, checkpoint cp\n    where cdc.filename like '%-merchant-%'\n      and cp.timestamp = (select min(timestamp)\n                          from checkpoint\n                          where timestamp >= split_part(cdc.filename, '-', 1))\n      and file_timestamp <= cp.timestamp\n      and file_timestamp > (select max(timestamp)\n                            from checkpoint\n                            where timestamp < split_part(cdc.filename, '-', 1))\n\"\"\")\nmerc_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4310882d-23fa-475a-a0e9-011d48a9d05b",
   "metadata": {
    "language": "python",
    "name": "reduce_merchant_data",
    "codeCollapsed": false,
    "collapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# We'll reduce the dataframe to only inlcude the latest update\n# associated with a single record per each resolve timestamp\n\n# We need a pandas dataframe in order use the .loc and groupby functions\nmerc_pd = merc_df.to_pandas()\n\n# And get the latest record per ID and resolve time in a single file\nmerc_pd = merc_pd.loc[merc_pd\n    .groupby(\n        ['MERCH_ID', 'CP_RESOLVED', 'FILE_TIMESTAMP']\n    )['FILE_ROW_NUMBER']\n    .agg(pd.Series.idxmax)]\n\n# Then reduce to the latest record across multiple files\nmerc_pd = merc_pd.loc[merc_pd\n    .groupby(\n        ['MERCH_ID', 'CP_RESOLVED']\n    )['FILE_TIMESTAMP']\n    .agg(pd.Series.idxmax)]\n\n# Now our data should be unique by the entity ID and CP_RESOLVED\n# And we'll set the index to later join with transaction data\nmerc_pd.set_index(['MERCH_ID', 'CP_RESOLVED'], inplace=True)\n\nmerc_pd.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f119cb4-b172-4b98-9454-9ec48b0d86bd",
   "metadata": {
    "language": "python",
    "name": "read_transaction_data",
    "codeCollapsed": false,
    "collapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# Create a Snowpark DataFrame using a sql query to parse JSON fields\n# And match records to a resolved time based on ingested file timestamp\ntran_df = session.sql(\"\"\"\n    with checkpoint as (\n        select data:resolved::varchar(32) as resolved,\n               split_part(filename, '.', 1) as timestamp\n        from cdc_data\n        where filename like '%.RESOLVED'\n    )\n    select cdc.data:after:id::varchar(64) as tran_id,\n           cdc.data:after:cc_num::varchar(19) as cc_num,\n           cdc.data:after:merch_id::varchar(64) as merch_id,\n           cdc.data:after:trans_num::varchar(32) as trans_num,\n           cdc.data:after:trans_date::date as trans_date,\n           cdc.data:after:trans_time::time as trans_time,\n           cdc.data:after:unix_time::number(10, 0) as unix_time,\n           cdc.data:after:category::varchar(16) as category,\n           cdc.data:after:amt::number(12, 2) as amt,\n           cdc.data:after:is_fraud::boolean as is_fraud,\n           cp.resolved as cp_resolved,\n           cp.timestamp as cp_timestamp,\n           split_part(cdc.filename, '-', 1) as file_timestamp,\n           cdc.data:mvcc_timestamp::varchar(32) as mvcc_timestamp,\n           cdc.data:updated::varchar(32) as updated,\n           cdc.file_row_number as file_row_number\n    from cdc_data cdc, checkpoint cp\n    where cdc.filename like '%-transaction-%'\n      and cp.timestamp = (select min(timestamp)\n                          from checkpoint\n                          where timestamp >= split_part(cdc.filename, '-', 1))\n      and file_timestamp <= cp.timestamp\n      and file_timestamp > (select max(timestamp)\n                            from checkpoint\n                            where timestamp < split_part(cdc.filename, '-', 1))\n\"\"\")\ntran_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "574b8487-41e8-4087-82a0-75f0e9be3acd",
   "metadata": {
    "language": "python",
    "name": "join_customer_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "# Next we'll join the transaction data with our customer data fields\n# based on the resolved timestamps of the transactions and customer data\n\n# We need a pandas dataframe in order use the .loc and groupby functions\ntran_cust_pd = tran_df.to_pandas()\ntran_cust_pd = tran_cust_pd.join(\n    cust_pd,\n    on=['CC_NUM', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_CUST'\n)\n\n# Find customer keys without a matching customer record\n# i.e. the customer version has an earlier resolved timestamp\ncust_missing = tran_cust_pd.loc[\n    tran_cust_pd['SSN'].isnull()\n][['CC_NUM', 'CP_RESOLVED']].drop_duplicates(keep=False)\n\n# Then reset the index and generate customer records based on the\n# pervious version but with the required transaction resolve time\ncust_pd.reset_index(inplace=True)\nfor index, row in cust_missing.iterrows():\n    cust_data = cust_pd.query(\"\"\"\n        CC_NUM == '{}' and \\\n        CP_RESOLVED < '{}'\n    \"\"\".format(row['CC_NUM'], row['CP_RESOLVED']))\n    cust_data = cust_data.loc[\n        cust_data.groupby(['CC_NUM'])['CP_RESOLVED']\n        .agg(pd.Series.idxmax)\n    ]\n    for next, cust in cust_data.iterrows():\n        cust_pd.loc[len(cust_pd.index)] = {\n            'CC_NUM': cust['CC_NUM'],\n            'CP_RESOLVED': row['CP_RESOLVED'],\n            'CUST_ID': cust['CUST_ID'],\n            'SSN': cust['SSN'],\n            'FIRST': cust['FIRST'],\n            'LAST': cust['LAST'],\n            'GENDER': cust['GENDER'],\n            'JOB': cust['JOB'],\n            'DOB': cust['DOB'],\n            'ACCT_NUM': cust['ACCT_NUM'],\n            'PROFILE': cust['PROFILE'],\n            'CP_TIMESTAMP': cust['CP_TIMESTAMP'],\n            'FILE_TIMESTAMP': cust['FILE_TIMESTAMP'],\n            'MVCC_TIMESTAMP': cust['MVCC_TIMESTAMP'],\n            'UPDATED': cust['UPDATED'],\n            'FILE_ROW_NUMBER': cust['FILE_ROW_NUMBER'],\n        }\n\n# Now perform a second pass to join the transaction and customer data\ncust_pd.set_index(['CC_NUM', 'CP_RESOLVED'], inplace=True)\ntran_cust_pd = tran_df.to_pandas()\ntran_cust_pd = tran_cust_pd.join(\n    cust_pd,\n    on=['CC_NUM', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_CUST'\n)\n\n# And confirm that there's no more missing data\ncust_missing = tran_cust_pd.loc[\n    tran_cust_pd['SSN'].isnull()\n][['CC_NUM', 'CP_RESOLVED']].drop_duplicates(keep=False)\ncust_missing",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27f5c975-5692-41d4-852e-0391f32a4e95",
   "metadata": {
    "language": "python",
    "name": "join_address_data",
    "codeCollapsed": false,
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "# Next we'll join the transaction data with our address data fields\n# based on the resolved timestamps of the transactions and address data\n\n# We'll create a new pandas dataframe to get started\ntran_addr_pd = tran_cust_pd.join(\n    addr_pd,\n    on=['ACCT_NUM', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_ADDR'\n)\n\n# Find address keys without a matching address record\n# i.e. the address version has an earlier resolved timestamp\naddr_missing = tran_addr_pd.loc[\n    tran_addr_pd['STREET'].isnull()\n][['ACCT_NUM', 'CP_RESOLVED']].drop_duplicates(keep=False)\n\n# Then reset the index and generate address records based on the\n# pervious version but with the required transaction resolve time\naddr_pd.reset_index(inplace=True)\nfor index, row in addr_missing.iterrows():\n    addr_data = addr_pd.query(\"\"\"\n        ACCT_NUM == '{}' and \\\n        CP_RESOLVED < '{}'\n    \"\"\".format(row['ACCT_NUM'], row['CP_RESOLVED']))\n    addr_data = addr_data.loc[\n        addr_data.groupby(['ACCT_NUM'])['CP_RESOLVED']\n        .agg(pd.Series.idxmax)\n    ]\n    for next, addr in addr_data.iterrows():\n        addr_pd.loc[len(addr_pd.index)] = {\n            'ACCT_NUM': addr['ACCT_NUM'],\n            'CP_RESOLVED': row['CP_RESOLVED'],\n            'ADDR_ID': addr['ADDR_ID'],\n            'STREET': addr['STREET'],\n            'ZIP': addr['ZIP'],\n            'LAT': addr['LAT'],\n            'LNG': addr['LNG'],\n            'CP_TIMESTAMP': addr['CP_TIMESTAMP'],\n            'FILE_TIMESTAMP': addr['FILE_TIMESTAMP'],\n            'MVCC_TIMESTAMP': addr['MVCC_TIMESTAMP'],\n            'UPDATED': addr['UPDATED'],\n            'FILE_ROW_NUMBER': addr['FILE_ROW_NUMBER'],\n        }\n\n# Now perform a second pass to join the transaction and address data\naddr_pd.set_index(['ACCT_NUM', 'CP_RESOLVED'], inplace=True)\ntran_addr_pd = tran_cust_pd.join(\n    addr_pd,\n    on=['ACCT_NUM', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_ADDR'\n)\n\n# And confirm that there's no more missing data\naddr_missing = tran_addr_pd.loc[\n    tran_addr_pd['STREET'].isnull()\n][['ACCT_NUM', 'CP_RESOLVED']].drop_duplicates(keep=False)\naddr_missing",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6094999-5c6a-4eaf-a2c8-5911df742511",
   "metadata": {
    "language": "python",
    "name": "join_city_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# Next we'll join the transaction data with our city data fields\n# based on the resolved timestamps of the transactions and city data\n\n# We'll create a new pandas dataframe to get started\ntran_city_pd = tran_addr_pd.join(\n    city_pd,\n    on=['ZIP', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_CITY'\n)\n\n# Find city keys without a matching city record\n# i.e. the city version has an earlier resolved timestamp\ncity_missing = tran_city_pd.loc[\n    tran_city_pd['CITY'].isnull()\n][['ZIP', 'CP_RESOLVED']].drop_duplicates(keep=False)\n\n# Then reset the index and generate city records based on the\n# pervious version but with the required transaction resolve time\ncity_pd.reset_index(inplace=True)\nfor index, row in city_missing.iterrows():\n    city_data = city_pd.query(\"\"\"\n        ZIP == '{}' and \\\n        CP_RESOLVED < '{}'\n    \"\"\".format(row['ZIP'], row['CP_RESOLVED']))\n    city_data = city_data.loc[\n        city_data.groupby(['ZIP'])['CP_RESOLVED']\n        .agg(pd.Series.idxmax)\n    ]\n    for next, city in city_data.iterrows():\n        city_pd.loc[len(city_pd.index)] = {\n            'ZIP': city['ZIP'],\n            'CP_RESOLVED': row['CP_RESOLVED'],\n            'CITY_ID': city['CITY_ID'],\n            'CITY': city['CITY'],\n            'STATE': city['STATE'],\n            'CITY_POP': city['CITY_POP'],\n            'CP_TIMESTAMP': city['CP_TIMESTAMP'],\n            'FILE_TIMESTAMP': city['FILE_TIMESTAMP'],\n            'MVCC_TIMESTAMP': city['MVCC_TIMESTAMP'],\n            'UPDATED': city['UPDATED'],\n            'FILE_ROW_NUMBER': city['FILE_ROW_NUMBER'],\n        }\n\n# Now perform a second pass to join the transaction and city data\ncity_pd.set_index(['ZIP', 'CP_RESOLVED'], inplace=True)\ntran_city_pd = tran_addr_pd.join(\n    city_pd,\n    on=['ZIP', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_CITY'\n)\n\n# And confirm that there's no more missing data\ncity_missing = tran_city_pd.loc[\n    tran_city_pd['CITY'].isnull()\n][['ZIP', 'CP_RESOLVED']].drop_duplicates(keep=False)\ncity_missing",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f11b250-c394-4cec-bf78-0455233b6d0c",
   "metadata": {
    "language": "python",
    "name": "join_merhcant_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "# Next we'll join the transaction data with our merchant data fields\n# based on the resolved timestamps of the transactions and merchant data\n\n# We'll create a new pandas dataframe to get started\ntran_merc_pd = tran_city_pd.join(\n    merc_pd,\n    on=['MERCH_ID', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_MERC'\n)\n\n# Find merchant keys without a matching merchant record\n# i.e. the merchant version has an earlier resolved timestamp\nmerc_missing = tran_merc_pd.loc[\n    tran_merc_pd['MERCHANT'].isnull()\n][['MERCH_ID', 'CP_RESOLVED']].drop_duplicates(keep=False)\n\n# Then reset the index and generate merchant records based on the\n# pervious version but with the required transaction resolve time\nmerc_pd.reset_index(inplace=True)\nfor index, row in merc_missing.iterrows():\n    merc_data = merc_pd.query(\"\"\"\n        MERCH_ID == '{}' and \\\n        CP_RESOLVED < '{}'\n    \"\"\".format(row['MERCH_ID'], row['CP_RESOLVED']))\n    merc_data = merc_data.loc[\n        merc_data.groupby(['MERCH_ID'])['CP_RESOLVED']\n        .agg(pd.Series.idxmax)\n    ]\n    for next, merc in merc_data.iterrows():\n        merc_pd.loc[len(merc_pd.index)] = {\n            'MERCH_ID': merc['MERCH_ID'],\n            'CP_RESOLVED': row['CP_RESOLVED'],\n            'MERCHANT': merc['MERCHANT'],\n            'MERCH_LAT': merc['MERCH_LAT'],\n            'MERCH_LNG': merc['MERCH_LNG'],\n            'CP_TIMESTAMP': merc['CP_TIMESTAMP'],\n            'FILE_TIMESTAMP': merc['FILE_TIMESTAMP'],\n            'MVCC_TIMESTAMP': merc['MVCC_TIMESTAMP'],\n            'UPDATED': merc['UPDATED'],\n            'FILE_ROW_NUMBER': merc['FILE_ROW_NUMBER'],\n        }\n\n# Now perform a second pass to join the transaction and merchant data\nmerc_pd.set_index(['MERCH_ID', 'CP_RESOLVED'], inplace=True)\ntran_merc_pd = tran_city_pd.join(\n    merc_pd,\n    on=['MERCH_ID', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_MERC'\n)\n\n# Find merchant keys again without a matching merchant record\n# i.e. a transaction happened before the first merchant record resolved\nmerc_missing = tran_merc_pd.loc[\n    tran_merc_pd['MERCHANT'].isnull()\n][['MERCH_ID', 'CP_RESOLVED']].drop_duplicates(keep=False)\n\n# Then reset the index and generate merchant records based on the\n# earliest version but with the required transaction resolve time\nmerc_pd.reset_index(inplace=True)\nfor index, row in merc_missing.iterrows():\n    merc_data = merc_pd.query(\"\"\"\n        MERCH_ID == '{}' and \\\n        CP_RESOLVED >= '{}'\n    \"\"\".format(row['MERCH_ID'], row['CP_RESOLVED']))\n    merc_data = merc_data.loc[\n        merc_data.groupby(['MERCH_ID'])['CP_RESOLVED']\n        .agg(pd.Series.idxmin)\n    ]\n    for next, merc in merc_data.iterrows():\n        merc_pd.loc[len(merc_pd.index)] = {\n            'MERCH_ID': merc['MERCH_ID'],\n            'CP_RESOLVED': row['CP_RESOLVED'],\n            'MERCHANT': merc['MERCHANT'],\n            'MERCH_LAT': merc['MERCH_LAT'],\n            'MERCH_LNG': merc['MERCH_LNG'],\n            'CP_TIMESTAMP': merc['CP_TIMESTAMP'],\n            'FILE_TIMESTAMP': merc['FILE_TIMESTAMP'],\n            'MVCC_TIMESTAMP': merc['MVCC_TIMESTAMP'],\n            'UPDATED': merc['UPDATED'],\n            'FILE_ROW_NUMBER': merc['FILE_ROW_NUMBER'],\n        }\n\n# Now perform a third pass to join the transaction and merchant data\nmerc_pd.set_index(['MERCH_ID', 'CP_RESOLVED'], inplace=True)\ntran_merc_pd = tran_city_pd.join(\n    merc_pd,\n    on=['MERCH_ID', 'CP_RESOLVED'],\n    how='left',\n    rsuffix='_MERC'\n)\n\n# And confirm that there's no more missing data\nmerc_missing = tran_merc_pd.loc[\n    tran_merc_pd['MERCHANT'].isnull()\n][['MERCH_ID', 'CP_RESOLVED']].drop_duplicates(keep=False)\nmerc_missing",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdf79e88-810a-4e22-b6ef-1a4526d60c9e",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 296
   },
   "outputs": [],
   "source": "print(\"Raw Data: {}\".format(tran_df.count()))\nprint(\"Resolved: {}\".format(tran_merc_pd.shape[0]))\n\ntran_merc_pd.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f72db-d8b4-4f25-aaf0-928475009895",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "describe_transaction_data",
    "collapsed": false,
    "resultHeight": 357
   },
   "outputs": [],
   "source": "# make a copy of the transaction data and cleanup columns\ntran_pd = tran_merc_pd.copy().drop(columns=[\n    \"TRAN_ID\",\n    \"MERCH_ID\",\n    \"TRANS_NUM\",\n    \"CP_RESOLVED\",\n    \"CP_TIMESTAMP\",\n    \"FILE_TIMESTAMP\",\n    \"MVCC_TIMESTAMP\",\n    \"UPDATED\",\n    \"FILE_ROW_NUMBER\",\n    \"CUST_ID\",\n    \"CP_TIMESTAMP_CUST\",\n    \"FILE_TIMESTAMP_CUST\",\n    \"MVCC_TIMESTAMP_CUST\",\n    \"UPDATED_CUST\",\n    \"FILE_ROW_NUMBER_CUST\",\n    \"ADDR_ID\",\n    \"CP_TIMESTAMP_ADDR\",\n    \"FILE_TIMESTAMP_ADDR\",\n    \"MVCC_TIMESTAMP_ADDR\",\n    \"UPDATED_ADDR\",\n    \"FILE_ROW_NUMBER_ADDR\",\n    \"CITY_ID\",\n    \"CP_TIMESTAMP_CITY\",\n    \"FILE_TIMESTAMP_CITY\",\n    \"MVCC_TIMESTAMP_CITY\",\n    \"UPDATED_CITY\",\n    \"FILE_ROW_NUMBER_CITY\",\n    \"CP_TIMESTAMP_MERC\",\n    \"FILE_TIMESTAMP_MERC\",\n    \"MVCC_TIMESTAMP_MERC\",\n    \"UPDATED_MERC\",\n    \"FILE_ROW_NUMBER_MERC\"\n])\n\n# Look at descriptive stats on the DataFrame\ntran_pd.describe()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8babcdbe-083d-4e4e-b7ec-ac86982e775d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_clean_data",
    "resultHeight": 113
   },
   "source": [
    "### Data cleaning\n",
    "\n",
    "First, let's force headers to uppercase using Snowpark DataFrame operations for standardization when columns are later written to a Snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0ce8f-0f7d-4cc9-ad07-f59d5c8e67a2",
   "metadata": {
    "language": "python",
    "name": "uppercase_headers",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# Force headers to uppercase\ntran_df = session.create_dataframe(tran_pd)\nfor colname in tran_df.columns:\n    new_colname = str.upper(colname)\n    tran_df = tran_df.with_column_renamed(colname, new_colname)\n\ntran_df.limit(5)"
  },
  {
   "cell_type": "markdown",
   "id": "654c8a95-271b-440b-a579-8f42d8f6f135",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_schema_check",
    "resultHeight": 41
   },
   "source": [
    "Check the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ec169-24e9-477f-8170-a5c332249269",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "check_schema",
    "collapsed": false,
    "resultHeight": 786
   },
   "outputs": [],
   "source": "list(tran_df.schema)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54aea408-5f24-4fc1-9add-de83caed2b05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_write_data",
    "resultHeight": 46
   },
   "source": [
    "### Write cleaned data to a Snowflake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00d0c5-157e-461b-b3e7-6577e081935d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "write_data",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "tran_df.write.mode('overwrite').save_as_table('transaction')"
  },
  {
   "cell_type": "markdown",
   "id": "57a0fb14-608b-49c0-87e8-721c71cb5688",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_next_nb",
    "resultHeight": 67
   },
   "source": [
    "In the next notebook, we will perform data transformations with the Snowpark ML Preprocessing API for feature engineering. "
   ]
  }
 ]
}